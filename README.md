# dbpolicy_crawl
## 1. 项目介绍 

**社会政策数据库爬虫项目**

本项目基于**Puppeteer**的Python接口pyppeteer实现高并发浏览器爬取，并通过**Redis**实现分布式，将**子链接获取**和**子链接爬取**分开进行，利用多台服务器和爬虫代理实现高可靠性的高速爬取。

## 2.需求分析

本项目属于**新闻政策类**的爬虫项目，其解决的**需求痛点**主要在于以下几个方面：

1. 成千上万个站点（政府网站信息公开），内容和结构都不相同，采集器如何从这些风格迥异的HTML文件中提取一些结构化的信息：标题、时间、子链接、正文、发布源、组配分类等。
2. 日数据量和总数据量巨大，如何提高采集器的通用性和可靠性，监控政策发布（政府信息公开网站出台新信息），并持久化存储在数据库中。
3. 对于政策新闻类的网页，如何实现文本的智能化提取。
4. 对于不同网站的不同情况，需要实现一系列中间件来应对，诸如：政策文件以附件形式出现（*.pdf、.doc、.txt、 .jpg等*）
5. 一个如此大规模的爬虫如何分布式部署，并定时或按需启动，实现开发运维的一体化。

## 3.软件设计

内容太多，先省略，后面慢慢补充。

## 4.项目架构

本项目主要有以下几个文件组成

```python
|-dbpolicy_crawl
|--begin.py           # 爬虫程序入口文件
|--config.py          # 所有配置信息
|--database.py        # 操作数据库的工具文件
|--request_tools.py   # 发起request请求的工具文件
|--utils.py           # 所有软件运行中需要用到的工具代码
|--schedule.py		 # 主要的功能模块，实现爬虫调度
|--frame.py			 # 解析网页frame
|--parse_detail.py	 # 解析子链接
|--parse_context.py	 # 网页正文内容的自动化提取
|--README.md			 # readme文件
```

## 5.下一步计划

- [ ] 将ES的操作代码加入到database.py中

- [ ] 实现一个middleware.py的中间件文件来实现附件的判断、提取等功能

  ```shell
  Input：网站HTML文本
  Function: 判断该网页中是否含有附件，并判断是否需要用附件来替换原文（综合考虑原文内容长度和提           取结果，设置规则），若需要，返回附件提取后的内容，否则返回原文。
  Output：string字符串，原文或提取结果
  ```

  

- [ ] 将正文文本识别算法进行替换，使用开源项目**GeneralNewsExtractor**